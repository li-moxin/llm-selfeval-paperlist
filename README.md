# llm-selfeval-paperlist
2023-11-25: Updated some paper. 

A collection of paper about self-evaluation, self-correction, and hallucination mitigation of large language models. 

-------------------------------------

- On the Calibration of Large Language Models and Alignment
[Paper](https://arxiv.org/pdf/2311.13240.pdf)

- Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus
[Paper](https://arxiv.org/pdf/2311.13230.pdf)

- Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting
[Paper](https://arxiv.org/pdf/2311.13314.pdf)

- Calibrating Language Models via Augmented Prompt Ensembles
[Paper](https://openreview.net/pdf?id=L0dc4wqbNs)

- Towards A Unified View of Answer Calibration for Multi-Step Reasoning
[Paper](https://arxiv.org/abs/2311.09101)

- Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained Actionable Feedback
[Paper](https://arxiv.org/pdf/2311.09336.pdf)

- Mindâ€™s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models
[Paper](https://arxiv.org/abs/2311.09214)

- LLMs cannot find reasoning errors, but can correct them!
[Paper](https://arxiv.org/abs/2311.08516)

- Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method
[Paper](https://paperswithcode.com/paper/knowing-what-llms-do-not-know-a-simple-yet)

- EMPIRICAL EVALUATION OF UNCERTAINTY QUANTIFICATION IN RETRIEVAL-AUGMENTED LANGUAGE MODELS FOR SCIENCE
[Paper](https://arxiv.org/pdf/2311.09358.pdf)

- Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling
[Paper](https://arxiv.org/abs/2311.08718)

- Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?
[Paper](https://arxiv.org/abs/2311.08718)

- A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning
[Paper](https://arxiv.org/abs/2311.07954)

- Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey
[Paper](https://arxiv.org/abs/2311.07914)

- Can Large Language Models Really Improve by Self-critiquing Their Own Plans?
[Paper](https://arxiv.org/abs/2310.08118)

- Fine-tuning Language Models for Factuality
[Paper](https://arxiv.org/abs/2311.08401)

- GAINING WISDOM FROM SETBACKS: ALIGNING LARGE LANGUAGE MODELS VIA MISTAKE ANALYSIS
[Paper](https://arxiv.org/pdf/2310.10477)

- Learning From Mistakes Makes LLM Better Reasoner
[Paper](https://arxiv.org/abs/2310.20689)

- Learning from Mistakes via Cooperative Study Assistant for Large Language Models
[Paper](https://arxiv.org/abs/2305.13829)

- LITCAB: LIGHTWEIGHT CALIBRATION OF LANGUAGE MODELS ON OUTPUTS OF VARIED LENGTHS
[Paper](https://arxiv.org/abs/2310.19208)

- LLatrieval: LLM-Verified Retrieval for Verifiable Generation
[Paper](https://arxiv.org/abs/2311.07838)

- On Measuring Faithfulness of Natural Language Explanations
[Paper](https://arxiv.org/abs/2311.07466)

- Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration
[Paper](https://paperswithcode.com/paper/towards-reasoning-in-large-language-models)

- SELF-RAG: LEARNING TO RETRIEVE, GENERATE, AND CRITIQUE THROUGH SELF-REFLECTION
[Paper](https://arxiv.org/abs/2310.11511)

- SAC3: Reliable Hallucination Detection in Black-Box Language Models via
Semantic-aware Cross-check Consistency
[Paper](https://paperswithcode.com/paper/sac-3-reliable-hallucination-detection-in)

- LARGE LANGUAGE MODELS CANNOT SELF-CORRECT REASONING YET
[Paper](https://arxiv.org/abs/2310.01798)

- LLM Calibration and Automatic Hallucination Detection via Pareto Optimal Self-supervision
[Paper](https://paperswithcode.com/paper/automatic-calibration-and-error-correction/review/)

- Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models
[Paper](https://arxiv.org/abs/2307.10236)

- CHAIN-OF-VERIFICATION REDUCES HALLUCINATION IN LARGE LANGUAGE MODELS
[Paper](https://arxiv.org/abs/2309.11495)

- QUANTIFYING UNCERTAINTY IN ANSWERS FROM ANY LANGUAGE MODEL VIA INTRINSIC AND EXTRINSIC CONFIDENCE ASSESSMENT
[Paper](https://paperswithcode.com/paper/quantifying-uncertainty-in-answers-from-any)

- SELFCHECK: USING LLMS TO ZERO-SHOT CHECK THEIR OWN STEP-BY-STEP REASONING
[Paper](https://arxiv.org/pdf/2308.00436.pdf)

- Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models
[Paper](https://arxiv.org/abs/2307.01379)

- Uncertainty in Natural Language Generation: From Theory to Applications
[Paper](https://arxiv.org/abs/2307.15703)


