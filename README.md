# llm-selfeval-paperlist
2023-11-25: Updated some paper. 

A collection of paper about self-evaluation, self-correction, and hallucination mitigation of large language models. 


- On the Calibration of Large Language Models and Alignment (EMNLP 2023)
[Paper](https://arxiv.org/pdf/2311.13240.pdf)

- Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus (EMNLP 2023)
[Paper](https://arxiv.org/pdf/2311.13230.pdf)

- Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting
[Paper](https://arxiv.org/pdf/2311.13314.pdf)

- Calibrating Language Models via Augmented Prompt Ensembles
[Paper](https://openreview.net/pdf?id=L0dc4wqbNs)

- Towards A Unified View of Answer Calibration for Multi-Step Reasoning
[Paper](https://arxiv.org/abs/2311.09101)

- Pinpoint, Not Criticize: Refining Large Language Models via Fine-Grained Actionable Feedback
[Paper](https://arxiv.org/pdf/2311.09336.pdf)

- Mindâ€™s Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models
[Paper](https://arxiv.org/abs/2311.09214)

- LLMs cannot find reasoning errors, but can correct them!
[Paper](https://arxiv.org/abs/2311.08516)

- Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method
[Paper](https://paperswithcode.com/paper/knowing-what-llms-do-not-know-a-simple-yet)

- EMPIRICAL EVALUATION OF UNCERTAINTY QUANTIFICATION IN RETRIEVAL-AUGMENTED LANGUAGE MODELS FOR SCIENCE
[Paper](https://arxiv.org/pdf/2311.09358.pdf)

- Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling
[Paper](https://arxiv.org/abs/2311.08718)

- Deceiving Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?
[Paper](https://arxiv.org/abs/2311.08718)







